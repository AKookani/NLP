{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AKookani/NLP/blob/main/Q5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmz2z5YJ5sYT"
      },
      "source": [
        "# Q1: Sentiment Analysis with Naive Bayes Classifier(50 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSn4uR6c5sYc"
      },
      "source": [
        "**Objective:**\n",
        "\n",
        "You are tasked with implementing a Naive Bayes classifier for sentiment analysis. The provided code is incomplete, and your goal is to complete the missing parts. Additionally, you should train the classifier on a small dataset and analyze its performance.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1.**Complete the Code (35 points)**: Fill in the missing parts in the provided Python code for the Naive Bayes classifier. Pay special attention to the `extract_features` function.\n",
        "\n",
        "2.**Train and Test**: Train the Naive Bayes classifier on the training data and test it on a separate test set. Evaluate the accuracy of the classifier.\n",
        "\n",
        "3.**Analysis (15 points)**: Discuss the results. Identify any misclassifications and try to understand why the classifier may fail in those cases. Provide examples of sentences that were not predicted correctly and explain possible reasons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UnuksJ45sYh",
        "outputId": "29bec05d-a679-4b45-d581-826024afb1df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import random\n",
        "import math\n",
        "import string\n",
        "from collections import defaultdict\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import movie_reviews\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "E1nW6n3L5sYl"
      },
      "outputs": [],
      "source": [
        "def get_features(tokens):\n",
        "    # Remove punctuation\n",
        "    tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # Perform stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "yZGHLM8Y5sYn"
      },
      "outputs": [],
      "source": [
        "class NaiveBayesClassifier:\n",
        "    def __init__(self, classes):\n",
        "        \"\"\"\n",
        "        Initializes the classifier with necessary data structures.\n",
        "        :param classes: A set of possible class labels (e.g., {'pos', 'neg'}).\n",
        "        \"\"\"\n",
        "        self.classes = classes  # Stores class labels\n",
        "        self.class_probs = defaultdict(float)  # Prior probabilities of each class\n",
        "        self.feature_probs = defaultdict(lambda: defaultdict(float))  # Conditional probabilities of features\n",
        "        self.vocab = set()  # Set to store unique vocabulary (all unique words)\n",
        "\n",
        "    def train(self, training_data, smoothing=1.0):\n",
        "        \"\"\"\n",
        "        Trains the Naive Bayes classifier using the training data.\n",
        "        :param training_data: A list of (tokens, label) tuples where tokens are words from the text.\n",
        "        :param smoothing: Laplace smoothing factor (default is 1.0).\n",
        "        \"\"\"\n",
        "        # Dictionaries to count class occurrences and feature occurrences per class\n",
        "        class_counts = defaultdict(int)\n",
        "        feature_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "        # Iterate through the training data to populate counts\n",
        "        for tokens, label in training_data:\n",
        "            # Count the occurrences of each class\n",
        "            class_counts[label] += 1\n",
        "\n",
        "            # Extract features (cleaned tokens) and count their occurrences for the current class\n",
        "            features = get_features(tokens)\n",
        "            for feature in features:\n",
        "                self.vocab.add(feature)  # Add to the vocabulary\n",
        "                feature_counts[label][feature] += 1  # Increment count of the feature for the class\n",
        "\n",
        "        # Total number of training samples\n",
        "        total_samples = sum(class_counts.values())\n",
        "\n",
        "        # Compute prior probabilities for each class\n",
        "        self.class_probs = {label: count / total_samples for label, count in class_counts.items()}\n",
        "\n",
        "        # Compute conditional probabilities (with Laplace smoothing)\n",
        "        for label in self.classes:\n",
        "            # Total number of features (words) in the current class\n",
        "            total_features = sum(feature_counts[label].values())\n",
        "            for feature in self.vocab:\n",
        "                # Apply Laplace smoothing using the smoothing parameter\n",
        "                self.feature_probs[label][feature] = (\n",
        "                    (feature_counts[label][feature] + smoothing) / (total_features + smoothing * len(self.vocab))\n",
        "                )\n",
        "\n",
        "    def classify(self, features):\n",
        "        \"\"\"\n",
        "        Predicts the class of given features using the Naive Bayes formula.\n",
        "        :param features: A list of features (tokens) extracted from text.\n",
        "        :return: The predicted class label.\n",
        "        \"\"\"\n",
        "        # Dictionary to store log probabilities for each class\n",
        "        log_probs = {}\n",
        "\n",
        "        # Calculate log probabilities for each class\n",
        "        for label in self.classes:\n",
        "            # Start with the log of the prior probability of the class\n",
        "            log_prob = math.log(self.class_probs[label])\n",
        "            for feature in features:\n",
        "                # Add the log of the conditional probability of each feature (if in vocab)\n",
        "                if feature in self.vocab:\n",
        "                    log_prob += math.log(self.feature_probs[label][feature])\n",
        "            log_probs[label] = log_prob  # Store the total log probability for the class\n",
        "\n",
        "        # Return the class with the highest log probability\n",
        "        return max(log_probs, key=log_probs.get)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parameter Tuning Function**"
      ],
      "metadata": {
        "id": "UcBUMwcNKAqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tune_laplace_smoothing(training_data, test_data, smoothing_values):\n",
        "    \"\"\"\n",
        "    Tunes the Laplace smoothing parameter by evaluating the classifier's performance\n",
        "    on different values of the smoothing parameter.\n",
        "    :param training_data: The training dataset (list of (tokens, label) tuples).\n",
        "    :param test_data: The testing dataset (list of (tokens, label) tuples).\n",
        "    :param smoothing_values: A list of smoothing values to test.\n",
        "    :return: A list of tuples containing smoothing value, training accuracy, and testing accuracy.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for smoothing in smoothing_values:\n",
        "        # Create a new classifier instance\n",
        "        classifier = NaiveBayesClassifier(classes)\n",
        "\n",
        "        # Train the classifier with the current smoothing value\n",
        "        classifier.train(training_data, smoothing=smoothing)\n",
        "\n",
        "        # Calculate training and testing accuracy\n",
        "        train_accuracy = calculate_accuracy(training_data, classifier)\n",
        "        test_accuracy = calculate_accuracy(test_data, classifier)\n",
        "\n",
        "        # Append results\n",
        "        results.append((smoothing, train_accuracy, test_accuracy))\n",
        "    return results"
      ],
      "metadata": {
        "id": "vgv8U-HxJ-O9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "8m3l8ap95sYp"
      },
      "outputs": [],
      "source": [
        "# Load the movie reviews dataset from NLTK\n",
        "data = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "random.shuffle(data)\n",
        "\n",
        "# Shuffle the dataset for randomness\n",
        "random.shuffle(data)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(data) * split_ratio)\n",
        "train_set = data[:split_index]\n",
        "test_set = data[split_index:]\n",
        "\n",
        "# Train the Naive Bayes classifier\n",
        "classes = set(sentiment for _, sentiment in train_set)\n",
        "classifier = NaiveBayesClassifier(classes)\n",
        "classifier.train(train_set)\n",
        "\n",
        "def calculate_accuracy(dataset, classifier):\n",
        "    \"\"\"\n",
        "    Evaluates the accuracy of the given classifier on the specified dataset.\n",
        "    :param dataset: A list of (tokens, label) tuples.\n",
        "    :param classifier: The Naive Bayes classifier instance.\n",
        "    :return: The accuracy (fraction of correct predictions).\n",
        "    \"\"\"\n",
        "    correct_predictions = 0  # Counter for correct predictions\n",
        "\n",
        "    # Iterate over the dataset and classify each example\n",
        "    for example in dataset:\n",
        "        tokens, true_sentiment = example  # Extract tokens and true class label\n",
        "        features = get_features(tokens)  # Extract features from tokens\n",
        "        predicted_sentiment = classifier.classify(features)  # Predict the class label\n",
        "        if predicted_sentiment == true_sentiment:  # Check if the prediction is correct\n",
        "            correct_predictions += 1\n",
        "\n",
        "    # Calculate and return accuracy\n",
        "    return correct_predictions / len(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running Parameter Tuning**"
      ],
      "metadata": {
        "id": "vfNCuHG8LTdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_with_seed(seed, train_set, test_set, smoothing_values):\n",
        "    print(f\"\\nEvaluating with seed: {seed}\")\n",
        "    random.seed(seed)  # Set seed for reproducibility\n",
        "    random.shuffle(data)  # Shuffle dataset\n",
        "\n",
        "    # Split into train and test sets\n",
        "    split_index = int(len(data) * 0.8)\n",
        "    train_set = data[:split_index]\n",
        "    test_set = data[split_index:]\n",
        "\n",
        "    results = []\n",
        "    for smoothing in smoothing_values:\n",
        "        classifier = NaiveBayesClassifier(classes)\n",
        "        classifier.train(train_set, smoothing=smoothing)\n",
        "        train_acc = calculate_accuracy(train_set, classifier)\n",
        "        test_acc = calculate_accuracy(test_set, classifier)\n",
        "        results.append((smoothing, train_acc, test_acc))\n",
        "\n",
        "    print(\"Smoothing | Train Accuracy | Test Accuracy\")\n",
        "    for smoothing, train_acc, test_acc in results:\n",
        "        print(f\"{smoothing:.1f}       | {train_acc:.2f}          | {test_acc:.2f}\")\n",
        "\n",
        "# Test with multiple seeds and smoothing values\n",
        "smoothing_values = [0.5, 1.0, 2.0]\n",
        "seeds = [42, 99, 123]  # Example seeds for reproducibility\n",
        "\n",
        "for seed in seeds:\n",
        "    evaluate_with_seed(seed, data, data, smoothing_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVpI2I7kLNUN",
        "outputId": "0516814f-4f81-4a9d-9610-fca8144bfa1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating with seed: 42\n",
            "Smoothing | Train Accuracy | Test Accuracy\n",
            "0.5       | 0.98          | 0.83\n",
            "1.0       | 0.97          | 0.82\n",
            "2.0       | 0.96          | 0.83\n",
            "\n",
            "Evaluating with seed: 99\n",
            "Smoothing | Train Accuracy | Test Accuracy\n",
            "0.5       | 0.97          | 0.79\n",
            "1.0       | 0.97          | 0.81\n",
            "2.0       | 0.96          | 0.81\n",
            "\n",
            "Evaluating with seed: 123\n",
            "Smoothing | Train Accuracy | Test Accuracy\n",
            "0.5       | 0.97          | 0.81\n",
            "1.0       | 0.96          | 0.82\n",
            "2.0       | 0.96          | 0.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7urCyXi5sYs"
      },
      "source": [
        "# Q2 : Rule-Based and HMM-Based Tagging ( 50 Points )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-PfIeYJ5sYu"
      },
      "source": [
        "### Part-of-Speech Tagging with Rule-Based and Hidden Markov Models\n",
        "This notebook demonstrates how to implement a basic POS tagger. Students will complete sections to:\n",
        "1. Build a rule-based POS tagger.\n",
        "2. Implement a sequence model-based POS tagger using Hidden Markov Models (HMM).\n",
        "3. Evaluate the performance of both approaches.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV2M49N-5sYx"
      },
      "source": [
        "#### import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "kvmy-vDL5sYz"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict, Counter\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itnZXLN75sY0"
      },
      "source": [
        "#### load and explore dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "TKMMs_yU5sY1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f28ff57-f7c9-42de-81da-87e30f130c39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')], [('Rudolph', 'NOUN'), ('Agnew', 'NOUN'), (',', '.'), ('55', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), ('and', 'CONJ'), ('former', 'ADJ'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Consolidated', 'NOUN'), ('Gold', 'NOUN'), ('Fields', 'NOUN'), ('PLC', 'NOUN'), (',', '.'), ('was', 'VERB'), ('named', 'VERB'), ('*-1', 'X'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('British', 'ADJ'), ('industrial', 'ADJ'), ('conglomerate', 'NOUN'), ('.', '.')], [('A', 'DET'), ('form', 'NOUN'), ('of', 'ADP'), ('asbestos', 'NOUN'), ('once', 'ADV'), ('used', 'VERB'), ('*', 'X'), ('*', 'X'), ('to', 'PRT'), ('make', 'VERB'), ('Kent', 'NOUN'), ('cigarette', 'NOUN'), ('filters', 'NOUN'), ('has', 'VERB'), ('caused', 'VERB'), ('a', 'DET'), ('high', 'ADJ'), ('percentage', 'NOUN'), ('of', 'ADP'), ('cancer', 'NOUN'), ('deaths', 'NOUN'), ('among', 'ADP'), ('a', 'DET'), ('group', 'NOUN'), ('of', 'ADP'), ('workers', 'NOUN'), ('exposed', 'VERB'), ('*', 'X'), ('to', 'PRT'), ('it', 'PRON'), ('more', 'ADV'), ('than', 'ADP'), ('30', 'NUM'), ('years', 'NOUN'), ('ago', 'ADP'), (',', '.'), ('researchers', 'NOUN'), ('reported', 'VERB'), ('0', 'X'), ('*T*-1', 'X'), ('.', '.')], [('The', 'DET'), ('asbestos', 'NOUN'), ('fiber', 'NOUN'), (',', '.'), ('crocidolite', 'NOUN'), (',', '.'), ('is', 'VERB'), ('unusually', 'ADV'), ('resilient', 'ADJ'), ('once', 'ADP'), ('it', 'PRON'), ('enters', 'VERB'), ('the', 'DET'), ('lungs', 'NOUN'), (',', '.'), ('with', 'ADP'), ('even', 'ADV'), ('brief', 'ADJ'), ('exposures', 'NOUN'), ('to', 'PRT'), ('it', 'PRON'), ('causing', 'VERB'), ('symptoms', 'NOUN'), ('that', 'DET'), ('*T*-1', 'X'), ('show', 'VERB'), ('up', 'PRT'), ('decades', 'NOUN'), ('later', 'ADJ'), (',', '.'), ('researchers', 'NOUN'), ('said', 'VERB'), ('0', 'X'), ('*T*-2', 'X'), ('.', '.')]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Ensure the dataset is downloaded\n",
        "nltk.download('treebank')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "# Load the dataset\n",
        "sentences = treebank.tagged_sents(tagset='universal')\n",
        "\n",
        "# Task 1: Print the first 5 tagged sentences to understand the dataset\n",
        "print(sentences[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrXUx4oZ5sY2"
      },
      "source": [
        "#### preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "dVExhIFb5sY3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f1144dc-2ee8-4320-ade2-b8935e708d40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training sentences: 3131, Testing sentences: 783\n"
          ]
        }
      ],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "train_data, test_data = train_test_split(sentences, test_size=0.2, random_state=42)\n",
        "\n",
        "# Task 2: Print the number of training and testing sentences\n",
        "print(f\"Training sentences: {len(train_data)}, Testing sentences: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdeVt9SY5sY4"
      },
      "source": [
        "#### rule-based pos tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "ScyVDcSO5sY4"
      },
      "outputs": [],
      "source": [
        "# Define a rule-based tagger\n",
        "# Task 3: Fill in the rules and the default tagger\n",
        "patterns = [\n",
        "    (r'.*ing$', 'VERB'),    # Gerunds\n",
        "    (r'.*ed$', 'VERB'),     # Past tense verbs\n",
        "    (r'.*es$', 'VERB'),     # 3rd person singular present verbs\n",
        "    (r'.*ly$', 'ADV'),      # Adverbs\n",
        "    (r'.*\\'s$', 'NOUN'),    # Possessive nouns\n",
        "    (r'.*s$', 'NOUN'),      # Plural nouns\n",
        "    (r'^\\d+$', 'NUM'),      # Numbers\n",
        "    (r'.*', 'NOUN')         # Default: nouns\n",
        "]\n",
        "\n",
        "# Task 4: Implement the rule-based tagger using nltk.RegexpTagger\n",
        "rule_based_tagger = nltk.RegexpTagger(patterns)  # Replace 'None' with your solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuIzvM6W5sY5"
      },
      "source": [
        "#### evaluate rule-based tagger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "yuO3W04T5sY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b251da2-0dc2-4a15-c874-5aa68851a62e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Rule-Based Tagger: 0.33\n"
          ]
        }
      ],
      "source": [
        "from nltk.metrics import accuracy\n",
        "# Task 5\n",
        "\n",
        "# Extract the words (tokens) from the test data\n",
        "test_words = [[word for word, tag in sentence] for sentence in test_data] # Get words from each sentence\n",
        "\n",
        "# Extract the true tags (gold standard tags)\n",
        "test_tags = [[tag for word, tag in sentence] for sentence in test_data] # Get true tags from each sentence\n",
        "\n",
        "# Get the predicted tags from the rule-based tagger\n",
        "predicted_tags = [rule_based_tagger.tag(sentence) for sentence in test_words]\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy_rule_based = accuracy(\n",
        "    [tag for sent in test_tags for tag in sent], # Flatten the true tags\n",
        "    [tag for sent in predicted_tags for _, tag in sent] # Flatten the predicted tags\n",
        ")\n",
        "print(f\"Accuracy of Rule-Based Tagger: {accuracy_rule_based:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FI4Wj6T5sY6"
      },
      "source": [
        "#### HMM-based pos tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "ao471psU5sY8"
      },
      "outputs": [],
      "source": [
        "# Train an HMM-based tagger\n",
        "# Task 6: Extract tag and word sequences from the training data\n",
        "def preprocess_hmm_data(tagged_sentences):\n",
        "    \"\"\"\n",
        "    Extracts sequences of words and tags from tagged sentences.\n",
        "    \"\"\"\n",
        "    word_sequences = [[word for word, _ in sent] for sent in tagged_sentences]\n",
        "    tag_sequences = [[tag for _, tag in sent] for sent in tagged_sentences]\n",
        "    return word_sequences, tag_sequences\n",
        "\n",
        "train_words, train_tags = preprocess_hmm_data(train_data)\n",
        "test_words, test_tags = preprocess_hmm_data(test_data)\n",
        "\n",
        "# Train transition and emission probabilities\n",
        "tags = [tag for sentence in train_tags for tag in sentence]\n",
        "tag_freq = Counter(tags)\n",
        "tag_bigram_freq = Counter([tuple(sentence[i:i+2]) for sentence in train_tags for i in range(len(sentence)-1)])\n",
        "\n",
        "# Task 7: Transition probabilities\n",
        "def train_transition_probs(tag_bigram_freq, tag_freq):\n",
        "    \"\"\"\n",
        "    Compute transition probabilities P(tag2|tag1).\n",
        "    \"\"\"\n",
        "    transition_probs = {}\n",
        "    for bigram, count in tag_bigram_freq.items():\n",
        "      tag1, tag2 = bigram\n",
        "      transition_probs[(tag1, tag2)] = count / tag_freq[tag1] # Probability: count(tag1, tag2) / count(tag1)\n",
        "    return transition_probs\n",
        "\n",
        "# Task 8: Implement the emission probability training\n",
        "def train_emission_probs(tagged_sentences):\n",
        "    \"\"\"\n",
        "    Compute emission probabilities P(word|tag).\n",
        "    \"\"\"\n",
        "    emission_probs = defaultdict(Counter)\n",
        "    for sentence in tagged_sentences:\n",
        "        for word, tag in sentence:\n",
        "            emission_probs[tag][word] += 1 # Count word occurrences for each tag\n",
        "    for tag in emission_probs:\n",
        "        total = sum(emission_probs[tag].values()) # Total words tagged as 'tag'\n",
        "        for word in emission_probs[tag]:\n",
        "            emission_probs[tag][word] /= total # Normalize to get probabilities\n",
        "    return emission_probs\n",
        "\n",
        "# Calculate probabilities\n",
        "transition_probs = train_transition_probs(tag_bigram_freq, tag_freq)\n",
        "emission_probs = train_emission_probs(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTaqnseo5sY9"
      },
      "source": [
        "#### implement HMM decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "w14ehFYT5sY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ed17dd9-a91d-40a9-a87f-a7ffaed788e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: For the Agency for International Development , appropriators approved $ 200 million *U* in secondary loan guarantees under an expanded trade credit insurance program , and total loan guarantees for the Overseas Private Investment Corp. are increased *-3 by $ 40 million *U* over fiscal 1989 as part of the same Poland package .\n",
            "Predicted Tags: ['ADP', 'DET', 'NOUN', 'ADP', 'NOUN', 'NOUN', '.', 'PRON', 'VERB', '.', 'NUM', 'NUM', 'X', 'ADP', 'ADJ', 'NOUN', 'NOUN', 'ADP', 'DET', 'VERB', 'NOUN', 'NOUN', 'NOUN', 'NOUN', '.', 'CONJ', 'ADJ', 'NOUN', 'NOUN', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'VERB', 'X', 'ADP', '.', 'NUM', 'NUM', 'X', 'ADP', 'ADJ', 'NUM', 'ADP', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'NOUN', '.']\n"
          ]
        }
      ],
      "source": [
        "# Task 9: Implement Viterbi algorithm for decoding\n",
        "def viterbi_algorithm(sentence, transition_probs, emission_probs, tag_freq):\n",
        "    \"\"\"\n",
        "    Finds the most likely sequence of tags for a given sentence using the Viterbi algorithm.\n",
        "    \"\"\"\n",
        "    states = list(tag_freq.keys()) # All possible tags\n",
        "    n = len(sentence) # Length of the sentence\n",
        "    viterbi = np.zeros((len(states), n)) # Matrix to store probabilities\n",
        "    backpointer = np.zeros((len(states), n), dtype=int) # Matrix to store backpointers\n",
        "\n",
        "    # Initialization for the first word\n",
        "    for i, state in enumerate(states):\n",
        "        viterbi[i, 0] = (\n",
        "            transition_probs.get(('<s>', state), 1e-6) * # P(tag|start)\n",
        "            emission_probs[state].get(sentence[0], 1e-6) # P(word|tag)\n",
        "        )\n",
        "        backpointer[i, 0] = 0\n",
        "\n",
        "    # Recursion for the rest of the sentence\n",
        "    for t in range(1, n):\n",
        "        for i, state in enumerate(states):\n",
        "            max_prob = -1\n",
        "            max_state = 0\n",
        "            for j, prev_state in enumerate(states):\n",
        "                prob = (\n",
        "                    viterbi[j, t - 1] * # Previous probability\n",
        "                    transition_probs.get((prev_state, state), 1e-6) * # P(current_tag|prev_tag)\n",
        "                    emission_probs[state].get(sentence[t], 1e-6) # P(word|current_tag)\n",
        "                )\n",
        "                if prob > max_prob: # Update max probability and state\n",
        "                    max_prob = prob\n",
        "                    max_state = j\n",
        "            viterbi[i, t] = max_prob\n",
        "            backpointer[i, t] = max_state\n",
        "\n",
        "    # Backtrace to find the best tag sequence\n",
        "    best_path = []\n",
        "    last_state = np.argmax(viterbi[:, -1]) # Start with the tag having the highest probability\n",
        "    best_path.append(states[last_state])\n",
        "    for t in range(n - 1, 0, -1):\n",
        "        last_state = backpointer[last_state, t]\n",
        "        best_path.insert(0, states[last_state]) # Insert at the beginning\n",
        "\n",
        "    return best_path\n",
        "\n",
        "# Predict tags for a sentence\n",
        "test_sentence = test_words[0]\n",
        "predicted_tags = viterbi_algorithm(test_sentence, transition_probs, emission_probs, tag_freq)\n",
        "print(f\"Sentence: {' '.join(test_sentence)}\")\n",
        "print(f\"Predicted Tags: {predicted_tags}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_df0Cuaq5sY-"
      },
      "source": [
        "#### evaluate HMM-based tagger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "CshxHMbT5sY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cfd5a94-0bb3-4d5d-8581-53e2de13545b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of HMM-Based Tagger: 0.94\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the accuracy of the HMM-based tagger\n",
        "# Task 10: Implement accuracy calculation for HMM-based tagger\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for sentence, true_tags in zip(test_words, test_tags):\n",
        "    predicted_tags = viterbi_algorithm(sentence, transition_probs, emission_probs, tag_freq)\n",
        "    correct += sum(p == t for p, t in zip(predicted_tags, true_tags)) # Count matches\n",
        "    total += len(true_tags) # Total tags\n",
        "\n",
        "accuracy_hmm = correct / total\n",
        "print(f\"Accuracy of HMM-Based Tagger: {accuracy_hmm:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seed in [42, 123, 2024]:\n",
        "    train_data, test_data = train_test_split(sentences, test_size=0.2, random_state=seed)\n",
        "    train_words, train_tags = preprocess_hmm_data(train_data)\n",
        "    test_words, test_tags = preprocess_hmm_data(test_data)\n",
        "    transition_probs = train_transition_probs(tag_bigram_freq, tag_freq)\n",
        "    emission_probs = train_emission_probs(train_data)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for sentence, true_tags in zip(test_words, test_tags):\n",
        "        predicted_tags = viterbi_algorithm(sentence, transition_probs, emission_probs, tag_freq)\n",
        "        correct += sum(p == t for p, t in zip(predicted_tags, true_tags))\n",
        "        total += len(true_tags)\n",
        "\n",
        "    accuracy_hmm = correct / total\n",
        "    print(f\"Accuracy of HMM-Based Tagger with seed {seed}: {accuracy_hmm:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsV82fw0VPAh",
        "outputId": "45624a1c-1ab8-442b-ba70-b92e03071791"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of HMM-Based Tagger with seed 42: 0.94\n",
            "Accuracy of HMM-Based Tagger with seed 123: 0.93\n",
            "Accuracy of HMM-Based Tagger with seed 2024: 0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0u4L5Uy5sY_"
      },
      "source": [
        "# Submission Instructions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpicSKp55sY_"
      },
      "source": [
        "1.Submit a Google Colab notebook containing your completed code and experimentation results.\n",
        "\n",
        "2.Include comments and explanations in your code to help understand the implemented logic.\n",
        "\n",
        "3.Clearly present the results of your parameter tuning in the notebook.\n",
        "\n",
        "4.Provide a brief summary of your findings and insights in the conclusion section.\n",
        "\n",
        "**Additional Notes:**\n",
        "*   Ensure that the notebook runs successfully in Google Colab.\n",
        "*   Experiment with various seed texts to showcase the diversity of generated text.\n",
        "*   Document any issues encountered during experimentation and how you addressed them.\n",
        "\n",
        "**Grading:**\n",
        "*   Each task will be graded out of the specified points.\n",
        "*   Points will be awarded for correctness, clarity of code, thorough experimentation, and insightful analysis."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}